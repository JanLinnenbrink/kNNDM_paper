#-----------------------------------------------------------#
#             Simulation analysis 2: virtual species        #
#-----------------------------------------------------------#
#.libPaths("/home/j/jlinnenb/r_packages/")
library("parallel")
library("doParallel")
library("pbapply")
setwd("C:/git/kNNDM_paper/")
# Load utils, functions, and define number of iterations
source("code/simulation/sim_functions.R")
nsim <- 100
pboptions(type = "timer")
# Read data
spoly <- st_read(dsn="data/simulation/species_vdata.gpkg", layer="sampling_polygon")
wclim <- rast("data/simulation/species_stack.grd")
wgrid <- st_read(dsn="data/simulation/species_vdata.gpkg", layer="landscape_grid")
# Prepare parallelization
print(paste0("Process started with ", 11, " cores."))
cl <- makeCluster(detectCores()-1)
nsim <- 1
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
# Launch simulation
set.seed(1234)
sims <- pbreplicate(nsim, sim_species(wgrid, wclim, spoly), simplify=FALSE)
write_csv(do.call(rbind, sims), "results/sim_res.csv")
# We're done
stopCluster(cl)
rm("cl")
sims
source("code/simulation/im_utils.R")
for(nk in clustgrid$nk){
# Cut nk clusters
clust_nk <- stats::kmeans(tcoords, nk)$cluster
tabclust <- as.data.frame(table(clust_nk))
tabclust <- tabclust[order(tabclust$Freq, decreasing=T),]
tabclust$clust_k <- NA
# compute cluster centroids and apply PC loadings to shuffle along the 1st dimension
centr_tpoints <- sapply(tabclust$clust_nk, function(x){
centrpca <- matrix(apply(tcoords[clust_nk %in% x, , drop=FALSE], 2, mean), nrow = 1)
colnames(centrpca) <- colnames(tcoords)
return(predict(pcacoords, centrpca))
})
tabclust$centrpca <- centr_tpoints
tabclust <- tabclust[order(tabclust$centrpca),]
# We don't merge big clusters
clust_i <- 1
for(i in 1:nrow(tabclust)){
if(tabclust$Freq[i] >= nrow(tpoints)/k){
tabclust$clust_k[i] <- clust_i
clust_i <- clust_i + 1
}
}
rm("clust_i")
# And we merge the remaining into k groups
clust_i <- setdiff(1:k, unique(tabclust$clust_k))
tabclust$clust_k[is.na(tabclust$clust_k)] <- rep(clust_i, ceiling(nk/length(clust_i)))[1:sum(is.na(tabclust$clust_k))]
tabclust2 <- data.frame(ID = 1:length(clust_nk), clust_nk = clust_nk)
tabclust2 <- merge(tabclust2, tabclust, by = "clust_nk")
tabclust2 <- tabclust2[order(tabclust2$ID),]
clust_k <- tabclust2$clust_k
j <- j+1
print(j)
# Compute W statistic if not exceeding maxp
if(!any(table(clust_k)/length(clust_k)>maxp)){
Gjstar_i[[j]] <- distclust(tcoords, clust_k)
clustgrid$W[clustgrid$nk==nk] <- twosamples::wass_stat(Gjstar_i[[j]], Gij)
clustgroups[[paste0("nk", nk)]] <- clust_k
}
}
# keep every split of kNNDM, fig 2
k_plot <- function(k, maxp, tpoints, ppoints) {
# Gj: NN distance function for a cluster per point, i.e. LOO CV
tcoords <- sf::st_coordinates(tpoints)[,1:2]
Gj <- c(FNN::knn.dist(tcoords, k = 1))
# Gij: prediction to training NN distances
Gij <- c(FNN::knnx.dist(query = sf::st_coordinates(ppoints)[,1:2],
data = tcoords, k = 1))
# Build grid of number of clusters to try - we sample low numbers more intensively
clustgrid <- data.frame(nk = as.integer(round(exp(seq(log(k), log(nrow(tpoints)-1),
length.out = 6)))))
clustgrid$W <- NA
clustgrid <- clustgrid[!duplicated(clustgrid$nk),]
clustgroups=Gjstar_i <- list()
# Compute 1st PC for ordering clusters
pcacoords <- stats::prcomp(tcoords, center = TRUE, scale. = FALSE, rank = 1)
j = 0
for(nk in clustgrid$nk){
# Cut nk clusters
clust_nk <- stats::kmeans(tcoords, nk)$cluster
tabclust <- as.data.frame(table(clust_nk))
tabclust <- tabclust[order(tabclust$Freq, decreasing=T),]
tabclust$clust_k <- NA
# compute cluster centroids and apply PC loadings to shuffle along the 1st dimension
centr_tpoints <- sapply(tabclust$clust_nk, function(x){
centrpca <- matrix(apply(tcoords[clust_nk %in% x, , drop=FALSE], 2, mean), nrow = 1)
colnames(centrpca) <- colnames(tcoords)
return(predict(pcacoords, centrpca))
})
tabclust$centrpca <- centr_tpoints
tabclust <- tabclust[order(tabclust$centrpca),]
# We don't merge big clusters
clust_i <- 1
for(i in 1:nrow(tabclust)){
if(tabclust$Freq[i] >= nrow(tpoints)/k){
tabclust$clust_k[i] <- clust_i
clust_i <- clust_i + 1
}
}
rm("clust_i")
# And we merge the remaining into k groups
clust_i <- setdiff(1:k, unique(tabclust$clust_k))
tabclust$clust_k[is.na(tabclust$clust_k)] <- rep(clust_i, ceiling(nk/length(clust_i)))[1:sum(is.na(tabclust$clust_k))]
tabclust2 <- data.frame(ID = 1:length(clust_nk), clust_nk = clust_nk)
tabclust2 <- merge(tabclust2, tabclust, by = "clust_nk")
tabclust2 <- tabclust2[order(tabclust2$ID),]
clust_k <- tabclust2$clust_k
j <- j+1
print(j)
# Compute W statistic if not exceeding maxp
if(!any(table(clust_k)/length(clust_k)>maxp)){
Gjstar_i[[j]] <- distclust(tcoords, clust_k)
clustgrid$W[clustgrid$nk==nk] <- twosamples::wass_stat(Gjstar_i[[j]], Gij)
clustgroups[[paste0("nk", nk)]] <- clust_k
}
}
}
